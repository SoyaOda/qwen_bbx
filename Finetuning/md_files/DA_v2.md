0. ç›®çš„ã¨é©ç”¨ç¯„å›²

ç›®çš„ï¼šNutrition5k ã®ãƒˆãƒƒãƒ—ãƒ“ãƒ¥ãƒ¼é£Ÿå“ç”»åƒã«å¯¾ã—ã¦ã€DAV2â€‘Metric ã‚’çµ¶å¯¾ã‚¹ã‚±ãƒ¼ãƒ«ã§å¾®èª¿æ•´ã—ã€**é£Ÿå“ä½“ç©ï¼ˆmLï¼‰**ã‚’é«˜ç²¾åº¦ã«æ¨å®šã€‚

å‡ºåŠ›ï¼šå„ç”»åƒãƒ»å„ãƒã‚¹ã‚¯ã§ä½“ç©[mL]ã€è£œåŠ©ã¨ã—ã¦æ·±åº¦RMSEç­‰ã€‚

è©•ä¾¡ï¼šNutrition5k ã® holdâ€‘out ãƒ†ã‚¹ãƒˆã§ä½“ç© MAE / MAPEï¼ˆç›¸å¯¾èª¤å·®ï¼‰ã‚’ãƒ¬ãƒãƒ¼ãƒˆã€‚

1. ä¾å­˜é–¢ä¿‚ã¨ãƒ•ã‚©ãƒ«ãƒ€è¨­è¨ˆ
1.1 ä¾å­˜é–¢ä¿‚
# æ—¢å­˜venvã‚’æƒ³å®š
pip install -U torch torchvision timm einops opencv-python pillow albumentations
pip install -U transformers  # HFçµŒç”±ã§ã‚‚ä½¿ãˆã‚‹ãŒã€ä»Šå›ã¯å…¬å¼Repoæº–æ‹ ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå„ªå…ˆ
# Depth-Anything V2 æœ¬ä½“ï¼ˆå­¦ç¿’ã« metric_depth/ ã‚’ä½¿ã†ï¼‰
git clone https://github.com/DepthAnything/Depth-Anything-V2
pip install -r Depth-Anything-V2/requirements.txt


å…¬å¼Repoã« metric_depth/ å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚ã‚Šï¼ˆSiLogLossãƒ»å†ç¾æ‰‹é †ï¼‰ã€‚æœ¬è¨ˆç”»ã§ã¯ metric_depth/train.py ã®æ§‹æˆã«åˆã‚ã›ã¦å®Ÿè£…ã—ã¾ã™ã€‚
Hugging Face

1.2 æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªï¼ˆSoyaOda/qwen_bbxï¼‰ã¸ã®è¿½åŠ 
qwen_bbx/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ depthany/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model_dav2_metric.py              # DAV2-Metric ãƒ©ãƒƒãƒ‘
â”‚   â”‚   â”œâ”€â”€ dataset_nutrition5k_metric.py     # Nutrition5k Datasetï¼ˆRGB/Depth/Mask/K/Volumeï¼‰
â”‚   â”‚   â”œâ”€â”€ losses_metric_food.py             # SiLog + å‹¾é… + å¹³é¢ + ä½“ç©
â”‚   â”‚   â”œâ”€â”€ volume_ops.py                     # ä½“ç©è¨ˆç®—ãƒ»å¹³é¢å½“ã¦ã¯ã‚
â”‚   â”‚   â””â”€â”€ utils_n5k.py                      # å‰å‡¦ç†ãƒ»Kç”Ÿæˆãƒ»splitå‡¦ç†
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ n5k_precompute_masks_and_volumes.py   # ãƒã‚¹ã‚¯ç”Ÿæˆ(SAM2ç­‰, fallback depthæ³•)ã¨GTä½“ç©ä½œæˆ
â”‚   â”œâ”€â”€ train_dav2_metric_n5k.py              # å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ¬è¨ˆç”»ã®ä¸­æ ¸ï¼‰
â”‚   â””â”€â”€ eval_dav2_metric_n5k.py               # è©•ä¾¡ï¼ˆhold-outï¼‰
â””â”€â”€ checkpoints/
    â””â”€â”€ depth_anything_v2_metric_hypersim_vitl.pth  # äº‹å‰å­¦ç¿’é‡ã¿ï¼ˆIndoor-Largeï¼‰


DAV2â€‘Metric Indoor/Large ã®é…å¸ƒãƒšãƒ¼ã‚¸ï¼ˆHFï¼‰ã‹ã‚‰å–å¾—ï¼šDepthâ€‘Anythingâ€‘V2â€‘Metricâ€‘Hypersimâ€‘Largeã€‚
Hugging Face

READMEã®ã€ŒUse our modelsã€ç¯€ã‚‚å‚ç…§ï¼ˆViTâ€‘L æ§‹æˆã¨ãƒ­ãƒ¼ãƒ‰ä¾‹ï¼‰ã€‚
GitHub

2. Nutrition5k ã®ä»•æ§˜ â†’ K ã®æ±ºå®šã¨å˜ä½å¤‰æ›
2.1 æ·±åº¦å˜ä½ã¨ã‚«ãƒ¡ãƒ©æ¡ä»¶ï¼ˆå…¬å¼ï¼‰

depth_raw ã®å˜ä½ï¼š1eâ€‘4 mï¼ˆ= 1 m ãŒ 10,000ï¼‰ã€‚

ã‚«ãƒ¡ãƒ©â€“ãƒ†ãƒ¼ãƒ–ãƒ«è·é›¢ï¼š0.359 mã€‚

1 ãƒ”ã‚¯ã‚»ãƒ«é¢ç©ï¼š5.957Ã—10â»Â³ cmÂ² = 5.957Ã—10â»â· mÂ²ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«é¢ä¸Šï¼‰ã€‚

è§£åƒåº¦ï¼š640Ã—480ï¼ˆRealSense D435/415 ä¸Šä¸‹è¦–ç‚¹ï¼‰ã€‚
CVF Open Access

2.2 K ã®å°å‡ºï¼ˆ640Ã—480å‰æï¼‰

ãƒ†ãƒ¼ãƒ–ãƒ«é¢ï¼ˆZ=0.359mï¼‰ã§ã® 1pix é¢ç© 
ğ‘
pix
=
ğ‘
2
/
(
ğ‘“
ğ‘¥
ğ‘“
ğ‘¦
)
a
pix
	â€‹

=Z
2
/(f
x
	â€‹

f
y
	â€‹

)ã€‚

ğ‘“
ğ‘¥
â‰ˆ
ğ‘“
ğ‘¦
f
x
	â€‹

â‰ˆf
y
	â€‹

 ã¨ä»®å®šã™ã‚‹ã¨ï¼š

ğ‘“
=
ğ‘
2
ğ‘
pix
=
0.359
2
5.957
Ã—
10
âˆ’
7
â‰ˆ
465.14
Â [px]
f=
a
pix
	â€‹

Z
2
	â€‹

	â€‹

=
5.957Ã—10
âˆ’7
0.359
2
	â€‹

	â€‹

â‰ˆ465.14Â [px]

ã‚ˆã£ã¦

ğ¾
=
[
465.14
	
0
	
320


0
	
465.14
	
240


0
	
0
	
1
]
K=
	â€‹

465.14
0
0
	â€‹

0
465.14
0
	â€‹

320
240
1
	â€‹

	â€‹


ã‚’ Nutrition5k ã®æ—¢å®š K ã¨ã—ã¦ç”¨ã„ã¾ã™ï¼ˆ640Ã—480å°‚ç”¨ï¼‰ã€‚
CVF Open Access

æ³¨ï¼šãƒ”ã‚¯ã‚»ãƒ«é¢ç©ã‹ã‚‰é€†ç®—ã—ãŸ 
ğ‘“
f ã¯è«–æ–‡ç”±æ¥ã§ã€Rawã®å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã‚‚æ•´åˆã€‚Nutrition5kã«ã¯å€‹åˆ¥ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¯é…å¸ƒã•ã‚Œã¦ã„ãªã„ãŸã‚ï¼ˆGitHub/è«–æ–‡è¨˜è¼‰ï¼‰ã€ã“ã®æ•´åˆçš„ãªå›ºå®šKã‚’ä½¿ã†ã®ãŒå®Ÿå‹™çš„ã€‚
CVF Open Access

3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰
3.1 ãƒ‡ãƒ¼ã‚¿é…ç½®ã¨Split
NUTRITION5K_ROOT/
â”œâ”€â”€ imagery/realsense_overhead/dish_XXXXXXXXXX/{rgb.png, depth_raw.png, depth_color.png}
â”œâ”€â”€ dish_ids/splits/{depth_train_ids.txt, depth_test_ids.txt}
â””â”€â”€ metadata/{dish_metadata_*.csv ...}


å…¬å¼é…å¸ƒã® split ã‚’åŸºæº–ã€‚æ¤œè¨¼(val)ã¯ train ã®10%ã‚’ãƒ©ãƒ³ãƒ€ãƒ ç¢ºä¿ï¼ˆå†ç¾æ€§seedå›ºå®šï¼‰ã€‚
CVF Open Access

3.2 ãƒã‚¹ã‚¯ç”Ÿæˆã¨GTä½“ç©ã®äº‹å‰è¨ˆç®—ï¼ˆæ¨å¥¨ï¼‰

æ—¢å­˜ã® QwenVLâ†’SAM2.1 ãƒ‘ã‚¤ãƒ—ã‚’ãƒãƒƒãƒåŒ–ã—ã¦é£Ÿå“ãƒã‚¹ã‚¯PNGã‚’ä½œæˆã€‚

ãƒã‚¹ã‚¯ãŒç„¡ã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ·±åº¦ãƒ™ãƒ¼ã‚¹

ãƒã‚¹ã‚¯å¤–ãƒªãƒ³ã‚°ã§å¹³é¢è¿‘ä¼¼ï¼ˆRANSACâ†’L2æœ€å°ï¼‰ã€‚

é«˜ã• 
â„
=
ğ‘
plane
âˆ’
ğ‘
(
ğ‘¥
,
ğ‘¦
)
h=Z
plane
	â€‹

âˆ’Z(x,y) ã‚’é–¾å€¤ï¼ˆä¾‹ï¼š2â€“3mmï¼‰ã§äºŒå€¤åŒ–ã—ã€ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã§é ˜åŸŸç¢ºå®šã€‚

GTä½“ç©ã¯ GTæ·±åº¦ï¼‹Kï¼‹é£Ÿå“ãƒã‚¹ã‚¯ã‹ã‚‰ç©åˆ†ï¼ˆÂ§5ã®å¼ï¼‰ã§mLã«å¤‰æ›ã—ã¦ CSV ã«ä¿å­˜ï¼ˆvolumes.csvï¼‰ã€‚

ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ‰ï¼š tools/n5k_precompute_masks_and_volumes.py

# é‡è¦: Nutrition5kã®å˜ä½ â†’ ãƒ¡ãƒ¼ãƒˆãƒ«ã«çµ±ä¸€
depth_m = (cv2.imread(depth_raw_path, -1).astype(np.float32)) * 1e-4  # 1e-4 m å˜ä½
K = np.array([[465.14,0,320],[0,465.14,240],[0,0,1]], np.float32)

# ãƒ†ãƒ¼ãƒ–ãƒ«å¹³é¢æ¨å®šï¼ˆãƒªãƒ³ã‚°é ˜åŸŸï¼‰â†’ n, d
# 1) ãƒã‚¹ã‚¯æ—¢å­˜: ring = dilate(mask, r=15)-mask
# 2) ç„¡ã„å ´åˆ: å‘¨ç¸20pxã‚’ãƒªãƒ³ã‚°ã¨ã—ã€å¤–ã‚Œå€¤é™¤å»ã®RANSACâ†’æœ€å°äºŒä¹—
# (å¹³é¢ã¯ z = ax + by + c ã‚’XYZåº§æ¨™ã§æœ€å°äºŒä¹—è§£)

# ä½“ç©(m^3) = Î£[ height(x,y) * a_pix(x,y) ] at mask
# height = max(0, z_plane(x,y) - z(x,y))
# a_pix = z^2 / (fx*fy)
volume_m3 = np.sum(height * (depth_m**2)/(K[0,0]*K[1,1]))
volume_ml = volume_m3 * 1e6


ï¼ˆNutrition5k ã® depth å˜ä½ãƒ»è·é›¢ãƒ»ãƒ”ã‚¯ã‚»ãƒ«é¢ç©ã®æ ¹æ‹ ã¯è«–æ–‡ãƒ»å…¬å¼READMEã«ä¾æ‹ ã€‚) 
CVF Open Access

4. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆDAV2â€‘Metric, Indoorâ€‘Largeï¼‰

äº‹å‰å­¦ç¿’é‡ã¿ï¼šHypersimâ€‘Large (ViTâ€‘L) ã‚’ä½¿ç”¨ï¼ˆIndoorå‘ã‘ï¼‰ã€‚

ä½¿ã„æ–¹ãƒ»æ§‹æˆãƒ»å†ç¾æ‰‹é †ï¼ˆHypersim/VKITTI2ï¼‰ã®å…¬é–‹ã‚ã‚Šã€‚
Hugging Face
+1

src/depthany/model_dav2_metric.py

import torch, cv2
from depth_anything_v2.dpt import DepthAnythingV2

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
MODEL_CFG = {'encoder':'vitl','features':256,'out_channels':[256,512,1024,1024]}

def load_dav2_metric_indoor_large(ckpt_path: str, max_depth: float = 2.0):
    model = DepthAnythingV2(**{**MODEL_CFG, 'max_depth': max_depth})
    state = torch.load(ckpt_path, map_location='cpu')
    model.load_state_dict(state, strict=True)
    model.to(DEVICE).eval()
    return model


max_depth ã¯ DAV2â€‘Metric å…¬å¼ãŒ 20mã‚’ä¾‹ç¤ºï¼ˆå®¤å†…ä¸Šé™ï¼‰ã€‚é£Ÿå“å“ä¸Šã«åˆã‚ã› 2.0 mç­‰ã«ä¸‹ã’ã¦ã‚ˆã„ï¼ˆãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¬ãƒ³ã‚¸æœ€é©åŒ–ï¼‰ã€‚
Hugging Face

5. ä½“ç©è¨ˆç®—ã®ç¢ºå®šå¼ï¼ˆå…±é€šï¼‰

src/depthany/volume_ops.py

import numpy as np

def pixel_area_map(depth_m, fx, fy):
    # a_pix = Z^2 / (fx * fy) [m^2 / px]
    return (depth_m ** 2) / (fx * fy)

def height_from_plane(depth_m, K, n, d):
    # ã‚«ãƒ¡ãƒ©åº§æ¨™ã¸æŠ•å½±ã—ã€å¹³é¢ã¨ã®å·®åˆ†ã‹ã‚‰é«˜ã•ã‚’ç®—å‡º
    H, W = depth_m.shape
    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]
    u, v = np.meshgrid(np.arange(W), np.arange(H))
    Z = depth_m
    X = (u - cx) * Z / fx
    Y = (v - cy) * Z / fy
    # å¹³é¢: nÂ·p + d = 0 ã¨ã—ã¦ã€p=(X,Y,Z)
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚ˆã‚Šä¸Š: h = max(0, -(nÂ·p + d) / ||n||)
    num = -(n[0]*X + n[1]*Y + n[2]*Z + d)
    h = np.maximum(0.0, num / np.linalg.norm(n))
    return h

def integrate_volume_ml(height_m, depth_m, K, mask):
    a_pix = pixel_area_map(depth_m, K[0,0], K[1,1])
    vol_m3 = np.sum(height_m[mask>0] * a_pix[mask>0])
    return float(vol_m3 * 1e6)  # mL

6. Dataset å®Ÿè£…ï¼ˆNutrition5k å°‚ç”¨ï¼‰

src/depthany/dataset_nutrition5k_metric.py

import os, cv2, json, random
import numpy as np
import albumentations as A
import torch
from torch.utils.data import Dataset

class Nutrition5kMetricDataset(Dataset):
    def __init__(self, root, split='train', ids_txt=None, use_masks=True, aug=False):
        self.root = root
        self.use_masks = use_masks
        self.ids = open(ids_txt).read().splitlines()
        # 640x480 å›ºå®šæƒ³å®šã®æ—¢å®šKï¼ˆÂ§2.2ï¼‰
        self.K = np.array([[465.14,0,320],[0,465.14,240],[0,0,1]], np.float32)
        self.aug = A.Compose([
            A.RandomRotate90(p=0.5),
            A.Flip(p=0.5),
            A.RandomResizedCrop(480, 640, scale=(0.85,1.0), ratio=(1.25,1.35), p=0.5),
            A.ColorJitter(0.1,0.1,0.1,0.05,p=0.3)
        ]) if aug else None

        # äº‹å‰è¨ˆç®—ã—ãŸä½“ç©ãƒ©ãƒ™ãƒ«ï¼ˆmLï¼‰
        self.vol_db = {}
        vol_csv = os.path.join(root, 'volumes.csv')
        if os.path.isfile(vol_csv):
            # dish_id,volume_ml ã®ç°¡æ˜“CSVã¨ä»®å®š
            for line in open(vol_csv):
                did, v = line.strip().split(',')
                self.vol_db[did] = float(v)

    def __len__(self): return len(self.ids)

    def _paths(self, did):
        base = os.path.join(self.root, 'imagery','realsense_overhead', f'dish_{did}')
        rgb = os.path.join(base, 'rgb.png')
        depth_raw = os.path.join(base, 'depth_raw.png')  # uint16, 1e-4 m
        mask = os.path.join(base, 'mask_food.png')       # toolsã§ç”Ÿæˆ
        return rgb, depth_raw, mask

    def __getitem__(self, i):
        did = self.ids[i]
        rgb_path, depth_path, mask_path = self._paths(did)

        img = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)
        depth_raw = cv2.imread(depth_path, -1).astype(np.float32)  # uint16
        depth_m = depth_raw * 1e-4                                 # â† é‡è¦ï¼ˆ1e-4 mï¼‰
        if self.use_masks and os.path.isfile(mask_path):
            mask = (cv2.imread(mask_path, 0) > 0).astype(np.uint8)
        else:
            mask = np.ones_like(depth_m, np.uint8)  # fallback

        if self.aug is not None:
            out = self.aug(image=img, mask=mask)
            img, mask = out['image'], out['mask']
            # depthã¯æœ€è¿‘å‚ã§åŒã‚¹ã‚±ãƒ¼ãƒ«ã«warpï¼ˆAã§ã¯ç‹¬è‡ªé©ç”¨ãŒå¿…è¦ï¼‰
            depth_m = cv2.resize(depth_m, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

        img_t = torch.from_numpy(img.transpose(2,0,1)).float()/255.0  # [3,H,W]
        depth_t = torch.from_numpy(depth_m).unsqueeze(0)              # [1,H,W]
        mask_t = torch.from_numpy(mask)                               # [H,W]
        K_t = torch.from_numpy(self.K)                                # [3,3]
        vol_ml = torch.tensor(self.vol_db.get(did, -1.0), dtype=torch.float32)
        return {'image':img_t, 'depth':depth_t, 'mask':mask_t, 'K':K_t, 'did':did, 'vol_ml':vol_ml}


depth_raw ã® 1eâ€‘4 må¤‰æ›ã¯å…¬å¼ä»•æ§˜ã€‚
CVF Open Access

7. æå¤±é–¢æ•°ï¼ˆSiLog + å‹¾é… + å¹³é¢ + ä½“ç©ï¼‰

src/depthany/losses_metric_food.py

import torch, torch.nn.functional as F

def silog_loss(pred, target, mask, eps=1e-6):
    # DAV2 metric_depth ã®æ¨™æº–ï¼ˆScale-invariant log RMSEï¼‰ã€‚å…¬å¼å­¦ç¿’ã§ã‚‚ä½¿ç”¨ã€‚ 
    # å‡ºå…¸: metric_depth/train.py ã¨åŒç­‰ã®å®Ÿè£…æ€æƒ³ã€‚  # å‚ç…§
    m = (mask>0).float()
    log_d = torch.log(pred.clamp_min(eps)) - torch.log(target.clamp_min(eps))
    log_d = log_d * m
    n = m.sum().clamp_min(1.0)
    mu = log_d.sum()/n
    return torch.sqrt(((log_d - mu)**2).sum()/n)

def gradient_loss(pred, target, mask):
    def grad_x(img): return img[:,:,:,1:] - img[:,:,:,:-1]
    def grad_y(img): return img[:,:,1:,:] - img[:,:,:-1,:]
    m = (mask>0).unsqueeze(1).float()
    gx = torch.abs(grad_x(pred) - grad_x(target)) * m[:,:,:,1:]
    gy = torch.abs(grad_y(pred) - grad_y(target)) * m[:,:,1:,:]
    n = m.sum().clamp_min(1.0)
    return (gx.sum()+gy.sum())/n

def plane_level_loss(pred_depth, K, ring_mask):
    # ãƒªãƒ³ã‚°é ˜åŸŸã‹ã‚‰å¹³é¢æ³•ç·š nz ã‚’æ¨å®š â†’ 1 - |nz|
    # è¨ˆç®—ã‚³ã‚¹ãƒˆä½ã®ãŸã‚ã€æœ€å°äºŒä¹—ã§è¿‘ä¼¼ï¼ˆãƒãƒƒãƒç°¡ç•¥åŒ–ï¼‰
    # ã“ã“ã§ã¯æ“¬ä¼¼çš„ã«é«˜ã•å‹¾é…ã®L1ã‚’ãƒªãƒ³ã‚°ã§æœ€å°åŒ–ï¼ˆå®‰å®šãƒ»è»½é‡ï¼‰
    m = (ring_mask>0).unsqueeze(1).float()
    gy = torch.abs(pred_depth[:,:,1:,:]-pred_depth[:,:,:-1,:]) * m[:,:,1:,:]
    gx = torch.abs(pred_depth[:,:,:,1:]-pred_depth[:,:,:,:-1]) * m[:,:,:,1:]
    n = m.sum().clamp_min(1.0)
    # å‹¾é…ãŒ0ã«è¿‘ã„ã»ã©æ°´å¹³ â†’ å¹³é¢æ€§æ­£å‰‡åŒ–
    return (gx.sum()+gy.sum())/n

def volume_loss(pred_depth, K, mask, vol_gt_ml, eps=1e-6):
    # ä½“ç©ã‚’ differentiable ã«è¿‘ä¼¼ï¼ˆa_pix=Z^2/(fx*fy)ï¼‰
    fx, fy = K[:,0,0], K[:,1,1]
    a_pix = (pred_depth**2) / (fx.view(-1,1,1).unsqueeze(1)*fy.view(-1,1,1).unsqueeze(1))
    # å¹³é¢ã¯åˆ¥é€”æ¨å®šã›ãšã€mask å†…ã®æœ€æ·±éƒ¨ã‚’ plane è¿‘ä¼¼ã™ã‚‹ç°¡æ˜“ç‰ˆã§ã‚‚OKï¼ˆé€Ÿåº¦é‡è¦–ï¼‰
    # ã“ã“ã§ã¯å®‰å…¨å´: äºˆã‚precomputeã—ãŸ plane ã‚’ä½¿ã†ãªã‚‰å¼•æ•°ã§å—ã‘ã‚‹
    # ç°¡ç•¥åŒ–: è² å€¤ã‚¯ãƒªãƒƒãƒ—æ¸ˆã¿é«˜ã• height>=0 ã‚’åˆ¥å‡¦ç†ã«ã—ã¦æ¸¡ã™è¨­è¨ˆã§ã‚‚ã‚ˆã„
    # å®Ÿè£…ç°¡æ½”åŒ–ã®ãŸã‚ã“ã“ã¯ L1 ã‚¹ã‚±ãƒ¼ãƒ«åˆã‚ã›ã®ã¿
    vol_pred_m3 = (a_pix*mask.unsqueeze(1)).sum(dim=[2,3]) * 0.0  # ã“ã“ã§ã¯ plane é«˜ã•åˆ¥é–¢æ•°ã«åˆ†é›¢æ¨å¥¨
    # â†’ å®Ÿé‹ç”¨ã¯ tools å´ã§precomputeã—ãŸ plane & height ã‚’å‚ç…§ã—å¾®åˆ†ä¸è¦åŒ–ã§OK
    vol_pred_ml = vol_pred_m3 * 1e6
    # ç›¸å¯¾èª¤å·®
    rel = torch.abs(vol_pred_ml - vol_gt_ml.view(-1,1).clamp_min(eps)) / vol_gt_ml.view(-1,1).clamp_min(eps)
    return rel.mean()


å­¦ç¿’ã®ä¸»æå¤±ã¯ SiLogLossï¼ˆå…¬å¼ metric_depth ã®æ¨™æº–ï¼‰ã§ã€å‹¾é…ãƒ»å¹³é¢ãƒ»ä½“ç©ã¯æ­£å‰‡åŒ–/è£œåŠ©ï¼ˆé‡ã¿ã¯å¾Œè¿°ï¼‰ã€‚å…¬å¼ metric_depth/train.py ã‚‚ SiLog ã‚’ä¸­å¿ƒã«æ§‹æˆã€‚
Hugging Face

8. å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰

tools/train_dav2_metric_n5k.py

import os, math, time, argparse
import numpy as np, torch
from torch.utils.data import DataLoader
from src.depthany.model_dav2_metric import load_dav2_metric_indoor_large
from src.depthany.dataset_nutrition5k_metric import Nutrition5kMetricDataset
from src.depthany.losses_metric_food import silog_loss, gradient_loss, plane_level_loss

def parse():
    ap = argparse.ArgumentParser()
    ap.add_argument('--root', type=str, required=True, help='N5k root')
    ap.add_argument('--ids-train', type=str, required=True)   # depth_train_ids.txt
    ap.add_argument('--ids-val', type=str, required=True)     # trainã®10%ã‚’æŠ½å‡º
    ap.add_argument('--ckpt', type=str, required=True)        # DAV2-Metric Indoor-Large
    ap.add_argument('--epochs', type=int, default=10)
    ap.add_argument('--bs', type=int, default=4)
    ap.add_argument('--lr', type=float, default=1e-4)
    ap.add_argument('--max-depth', type=float, default=2.0)   # å®¤å†…ã«æœ€é©åŒ–
    return ap.parse_args()

def main():
    args = parse()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # Dataset / Loader
    train_ds = Nutrition5kMetricDataset(args.root, split='train', ids_txt=args.ids-train, use_masks=True, aug=True)
    val_ds   = Nutrition5kMetricDataset(args.root, split='val', ids_txt=args.ids-val, use_masks=True, aug=False)
    train_ld = DataLoader(train_ds, batch_size=args.bs, shuffle=True, num_workers=4, pin_memory=True)
    val_ld   = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)

    # Model
    model = load_dav2_metric_indoor_large(args.ckpt, max_depth=args.max_depth)
    model.train()
    # ViTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã‚’æœ€åˆã¯å‡çµï¼ˆStage1ï¼‰
    for n,p in model.named_parameters():
        if 'pretrained' in n or 'encoder' in n:
            p.requires_grad = False

    optim = torch.optim.AdamW(filter(lambda p:p.requires_grad, model.parameters()), lr=args.lr)
    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))

    w_grad, w_plane = 0.1, 0.05   # åˆæœŸé‡ã¿
    for ep in range(args.epochs):
        model.train(); t0=time.time(); loss_ep=0.0
        for batch in train_ld:
            img = batch['image'].to(device)
            gt  = batch['depth'].to(device)
            msk = batch['mask'].to(device)
            K   = batch['K'].to(device)

            optim.zero_grad(set_to_none=True)
            with torch.cuda.amp.autocast(enabled=(device=='cuda')):
                pred = model(img) if hasattr(model,'forward') else model.infer_image(img)
                if isinstance(pred, dict) and 'predicted_depth' in pred:
                    pred = pred['predicted_depth']
                # shape [B,1,H,W]
                Ld = silog_loss(pred, gt, msk)
                Lg = gradient_loss(pred, gt, msk)
                # ãƒªãƒ³ã‚°ãƒã‚¹ã‚¯ã¯Datasetå´ã§åˆ¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã—ã¦ã‚‚è‰¯ã„ï¼ˆã“ã“ã§ã¯ç°¡æ˜“ã«å¢ƒç•Œè¿‘å‚ä½¿ç”¨ï¼‰
                Lp = plane_level_loss(pred, K, ring_mask=(1-msk))  # é£Ÿå“å¤–ã‚’ãƒªãƒ³ã‚°è¿‘ä¼¼
                loss = Ld + w_grad*Lg + w_plane*Lp

            scaler.scale(loss).backward()
            scaler.step(optim); scaler.update()
            loss_ep += loss.item()

        # ç°¡æ˜“Val
        model.eval(); mae=0.0; nimg=0
        with torch.no_grad():
            for batch in val_ld:
                img = batch['image'].to(device); gt = batch['depth'].to(device); msk = batch['mask'].to(device)
                pred = model(img); 
                if isinstance(pred, dict) and 'predicted_depth' in pred: pred = pred['predicted_depth']
                err = torch.abs(pred - gt)[msk>0].mean().item()
                mae += err; nimg += 1
        print(f'[EP{ep+1}] loss={loss_ep/len(train_ld):.4f} val|L1[mask]={mae/max(1,nimg):.4f} time={time.time()-t0:.1f}s')

    os.makedirs('checkpoints', exist_ok=True)
    torch.save(model.state_dict(), f'checkpoints/dav2_metric_n5k_vitl.pth')

if __name__ == '__main__':
    main()


å­¦ç¿’æ çµ„ã¿ã¯ DAV2â€‘Metric ã®å…¬å¼ metric_depth/train.pyï¼ˆSiLog ã‚’ä¸»æå¤±ã€å…¬å¼ã®å†ç¾ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰ã¨æ•´åˆã€‚ã¾ãšã¯Stage1ï¼šãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³å‡çµã§å®‰å®šåŒ–ã—ã€å¿…è¦ãªã‚‰ Stage2 ã§å…¨å±¤å­¦ç¿’ã¸ï¼ˆLR 1/2ï¼‰ã€‚
Hugging Face

9. è©•ä¾¡ï¼ˆholdâ€‘out ãƒ†ã‚¹ãƒˆã§ä½“ç©ï¼‰

tools/eval_dav2_metric_n5k.py

import os, csv, torch, numpy as np
from torch.utils.data import DataLoader
from src.depthany.model_dav2_metric import load_dav2_metric_indoor_large
from src.depthany.dataset_nutrition5k_metric import Nutrition5kMetricDataset
from src.depthany.volume_ops import height_from_plane, integrate_volume_ml

def eval_volume(root, ids_test, ckpt, max_depth=2.0):
    ds = Nutrition5kMetricDataset(root, split='test', ids_txt=ids_test, use_masks=True, aug=False)
    ld = DataLoader(ds, batch_size=1, shuffle=False)
    model = load_dav2_metric_indoor_large(ckpt, max_depth=max_depth); model.eval()
    rows=[]
    for b in ld:
        img, gt, mask, K, did = b['image'].cuda(), b['depth'].cuda(), b['mask'][0].cpu().numpy(), b['K'][0].cpu().numpy(), b['did'][0]
        with torch.no_grad():
            pred = model(img); 
            if isinstance(pred, dict) and 'predicted_depth' in pred: pred = pred['predicted_depth']
        pred_np = pred[0,0].cpu().numpy()

        # å¹³é¢ã¯ precompute æ¸ˆã¿ã§ã‚‚OKã€‚ã“ã“ã§ã¯ç°¡æ˜“ã« maskå¤–ãƒªãƒ³ã‚°ã§å†æ¨å®šã™ã‚‹é–¢æ•°ã‚’æµç”¨æƒ³å®š
        # n, d = fit_plane_from_ring(pred_np, K, ring)  # å®Ÿè£…ã¯ volume_ops ã¸
        # height = height_from_plane(pred_np, K, n, d)
        # vol_ml_pred = integrate_volume_ml(height, pred_np, K, mask)
        # GTä½“ç©ã¯ volumes.csv ã‹ã‚‰å–å¾—ï¼ˆãªã‘ã‚Œã° GTæ·±åº¦ã‹ã‚‰åŒæ§˜ã«è¨ˆç®—ï¼‰

        # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼: å®Ÿè£…ã§ã¯ precompute ã®é«˜ã•ãƒãƒƒãƒ—ã‚’ç”¨ã„ã‚‹
        vol_ml_pred = -1

        rows.append([did, vol_ml_pred])

    with open('eval_volume_pred.csv','w') as f:
        w=csv.writer(f); w.writerow(['dish_id','pred_volume_ml']); w.writerows(rows)

# å®Ÿè¡Œä¾‹:
# python tools/eval_dav2_metric_n5k.py --root data/Nutrition5k --ids-test dish_ids/splits/depth_test_ids.txt --ckpt checkpoints/dav2_metric_n5k_vitl.pth

10. å…ˆè¡Œã‚³ãƒ¼ãƒ‰ãƒ»è«–æ‹ ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ

Depthâ€‘Anything V2 æœ¬ä½“ãƒ»metric_depth ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ»å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ»SiLogLossãƒ»å†ç¾æ‰‹é †ï¼ˆHypersim/VKITTI2ï¼‰ã€‚
Hugging Face
+2
GitHub
+2

Indoor/Outdoor ã® Metric ãƒ¢ãƒ‡ãƒ«ï¼ˆSmall/Base/Largeï¼‰ é…å¸ƒã€‚Indoor ã¯ Hypersim ãƒ™ãƒ¼ã‚¹ã€‚ä»Šå›ã®èµ·ç‚¹ã¯ Metricâ€‘Hypersimâ€‘Largeã€‚
Hugging Face

Nutrition5k ã®æ·±åº¦å˜ä½ï¼ˆ1eâ€‘4 mï¼‰, 35.9 cm, 1pixé¢ç©ï¼ˆKå°å‡ºæ ¹æ‹ ï¼‰ã€‚
CVF Open Access

11. å­¦ç¿’ãƒ¬ã‚·ãƒ”ï¼ˆæ¨å¥¨å€¤ï¼‰

Stage1ï¼ˆ3â€“5 epochsï¼‰ï¼šViTãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³å‡çµã€LR=1eâ€‘4ã€BS=4ã€AMP æœ‰åŠ¹ã€‚

Stage2ï¼ˆ5â€“10 epochsï¼‰ï¼šå…¨å±¤å­¦ç¿’ã€LR=5eâ€‘5ã€w_grad=0.1, w_plane=0.05 ã®ã¾ã¾é–‹å§‹ã€‚

max_depthï¼š2.0mï¼ˆå“ä¸Šæ’®å½±å‘ã‘ï¼‰ã€‚

è©•ä¾¡æŒ‡æ¨™ï¼šä½“ç© MAE[mL], MAPE[%]ã€è£œåŠ©ã§æ·±åº¦ MAE[mm]ï¼ˆé£Ÿå“ãƒã‚¹ã‚¯å†…ï¼‰ã€‚

æ—©æœŸåœæ­¢ï¼šval ã®ä½“ç© MAPE ãŒ 3 ã‚¨ãƒãƒƒã‚¯æ”¹å–„ã—ãªã‘ã‚Œã°æ‰“ã¡åˆ‡ã‚Šã€‚

12. ã‚ˆãã‚ã‚‹è½ã¨ã—ç©´ã¨å¯¾ç­–

å˜ä½å¤‰æ›ãƒŸã‚¹ï¼šNutrition5k ã® 1eâ€‘4 m ã‚’å¿˜ã‚Œãªã„ã€‚
CVF Open Access

K ã®ä¸æ•´åˆï¼š640Ã—480ä»¥å¤–ã®ãƒªã‚µã‚¤ã‚ºã‚’å‰å‡¦ç†ã§è¡Œã£ãŸå ´åˆã¯ cx,cy ã‚‚ãƒªã‚µã‚¤ã‚ºå¾Œä¸­å¿ƒã«å†è¨­å®šã€‚

å¹³é¢æ¨å®šã®è² å€¤ã‚¯ãƒªãƒƒãƒ—ã§é«˜ã•0é€£ç™ºï¼šãƒªãƒ³ã‚°ãŒèƒŒæ™¯ã‚’å«ã‚€å ´åˆã€ã‚¨ãƒ­ãƒ¼ã‚¸ãƒ§ãƒ³/ãƒ€ã‚¤ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§é£Ÿå“ç¸ã‹ã‚‰ååˆ†é›¢ã™ã€‚

å­¦ç¿’åˆæœŸã®ä½“ç©ãƒ­ã‚¹ä¸å®‰å®šï¼šã¾ãš SiLog + å‹¾é… + å¹³é¢ã«é›†ä¸­ã—ã€ä½“ç©ãƒ­ã‚¹ã¯å¾ŒåŠã‹ã‚‰æ®µéšçš„ã«æœ‰åŠ¹åŒ–ã™ã‚‹ã®ã‚‚æœ‰åŠ¹ï¼ˆå…¬å¼ã¯SiLogä¸­å¿ƒï¼‰ã€‚
Hugging Face

13. è¿½åŠ ï¼šã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¤œè¨¼ï¼ˆFineâ€‘tuningå‰ï¼‰

DAV2â€‘Metric Indoorâ€‘Large ã®æ¨è«–ã‚’æ—¢å­˜ test_images/ï¼ˆtrain_00000.jpg ç­‰ï¼‰ã§å›ã™ ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆK ã¯ iPhone ã® EXIF or å›ºå®šæ—¢å®šï¼‰ã€‚

å…¬å¼ README ã®æ¨è«–ä¾‹ã¨ inputâ€‘size=518 ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æº–æ‹ ï¼ˆOpenCVã®ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å·®åˆ†ã‚ã‚Šï¼‰ã€‚
GitHub
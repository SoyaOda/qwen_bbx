#!/usr/bin/env python3
"""
EXIFæƒ…å ±ã‚’æŒã¤30æšã®ç”»åƒã§ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
"""
import os
import json
import shutil
from pathlib import Path
from PIL import Image
from PIL.ExifTags import TAGS
import csv

def has_exif(image_path):
    """ç”»åƒãŒEXIFæƒ…å ±ã‚’æŒã¤ã‹ç¢ºèª"""
    try:
        image = Image.open(image_path)
        exifdata = image.getexif()
        return exifdata is not None and len(exifdata) > 0
    except:
        return False

def get_exif_data(image_path):
    """ç”»åƒã‹ã‚‰EXIFæƒ…å ±ã‚’å–å¾—"""
    try:
        image = Image.open(image_path)
        exifdata = image.getexif()
        
        if not exifdata:
            return None
        
        # EXIFæƒ…å ±ã‚’èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        exif_dict = {}
        for tag_id, value in exifdata.items():
            tag = TAGS.get(tag_id, tag_id)
            # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã¯çŸ­ç¸®è¡¨ç¤º
            if isinstance(value, bytes):
                value = f"<binary data {len(value)} bytes>"
            exif_dict[tag] = str(value)
            
        return exif_dict
    except Exception as e:
        return None

def create_exif_test_dataset():
    """EXIFä»˜ãç”»åƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
    
    # ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    source_dir = Path("data/unimib2016/pre8/pre8")
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    test_dataset_dir = Path("data/exif_test_dataset")
    test_dataset_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("EXIFä»˜ããƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    print("=" * 60)
    
    # EXIFæƒ…å ±ã‚’æŒã¤ç”»åƒã‚’æ¤œç´¢
    print("ğŸ” EXIFæƒ…å ±ã‚’æŒã¤ç”»åƒã‚’æ¤œç´¢ä¸­...")
    
    exif_images = []
    all_images = list(source_dir.rglob('*.jpg')) + list(source_dir.rglob('*.JPG'))
    
    for img_path in all_images:
        if has_exif(img_path):
            exif_data = get_exif_data(img_path)
            if exif_data:
                exif_images.append({
                    'source_path': img_path,
                    'exif': exif_data
                })
                if len(exif_images) >= 30:
                    break
    
    print(f"âœ… {len(exif_images)}æšã®EXIFä»˜ãç”»åƒã‚’ç™ºè¦‹")
    
    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
    print("\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆä¸­...")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ãƒªã‚¹ãƒˆ
    metadata = []
    
    for i, img_info in enumerate(exif_images, 1):
        source_path = img_info['source_path']
        
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆé€£ç•ªï¼‰
        new_filename = f"exif_image_{i:03d}.jpg"
        dest_path = test_dataset_dir / new_filename
        
        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
        shutil.copy2(source_path, dest_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        metadata.append({
            'id': i,
            'filename': new_filename,
            'original_path': str(source_path.relative_to(source_dir)),
            'exif': img_info['exif']
        })
        
        print(f"  [{i:2d}/30] {source_path.name} â†’ {new_filename}")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§ä¿å­˜
    metadata_path = test_dataset_dir / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_path}")
    
    # EXIFæƒ…å ±ã‚µãƒãƒªãƒ¼ã‚’CSVã§ä¿å­˜
    csv_path = test_dataset_dir / "exif_summary.csv"
    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['ID', 'ãƒ•ã‚¡ã‚¤ãƒ«å', 'ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‘ã‚¹', 'EXIF ã‚¿ã‚°æ•°', 'ä¸»è¦ãªEXIFã‚¿ã‚°'])
        
        for item in metadata:
            main_tags = []
            for tag in ['Orientation', 'Make', 'Model', 'FocalLength', 'DateTimeOriginal']:
                if tag in item['exif']:
                    main_tags.append(f"{tag}={item['exif'][tag]}")
            
            writer.writerow([
                item['id'],
                item['filename'],
                item['original_path'],
                len(item['exif']),
                '; '.join(main_tags) if main_tags else 'N/A'
            ])
    
    print(f"ğŸ’¾ CSVã‚µãƒãƒªãƒ¼ä¿å­˜: {csv_path}")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º
    print("\n" + "=" * 60)
    print("ğŸ“Š ä½œæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print(f"  å ´æ‰€: {test_dataset_dir}")
    print(f"  ç”»åƒæ•°: {len(metadata)}æš")
    print(f"  ã™ã¹ã¦ã®ç”»åƒã«EXIFæƒ…å ±ä»˜ã")
    
    # EXIFæƒ…å ±ã®çµ±è¨ˆ
    all_tags = set()
    for item in metadata:
        all_tags.update(item['exif'].keys())
    
    print(f"\nğŸ“· EXIFæƒ…å ±ã®æ¦‚è¦:")
    print(f"  æ¤œå‡ºã•ã‚ŒãŸEXIFã‚¿ã‚°ç¨®é¡: {len(all_tags)}")
    print(f"  ä¸»ãªã‚¿ã‚°: {', '.join([str(tag) for tag in list(all_tags)[:10]])}")
    
    return test_dataset_dir

def verify_test_dataset(dataset_dir):
    """ä½œæˆã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œè¨¼"""
    print("\n" + "=" * 60)
    print("ğŸ” ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼")
    print("=" * 60)
    
    dataset_path = Path(dataset_dir)
    images = list(dataset_path.glob('*.jpg'))
    
    exif_count = 0
    for img_path in images:
        if has_exif(img_path):
            exif_count += 1
    
    print(f"  ç·ç”»åƒæ•°: {len(images)}")
    print(f"  EXIFä»˜ã: {exif_count}/{len(images)} ({exif_count/len(images)*100:.1f}%)")
    
    if exif_count == len(images):
        print("  âœ… ã™ã¹ã¦ã®ç”»åƒã«EXIFæƒ…å ±ãŒä¿æŒã•ã‚Œã¦ã„ã¾ã™")
    else:
        print("  âš ï¸ ä¸€éƒ¨ã®ç”»åƒã§EXIFæƒ…å ±ãŒå¤±ã‚ã‚Œã¦ã„ã¾ã™")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    test_dataset_dir = create_exif_test_dataset()
    
    # æ¤œè¨¼
    verify_test_dataset(test_dataset_dir)
    
    print("\nâœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†ï¼")
    print(f"   ä½¿ç”¨æ–¹æ³•: python check_exif.py {test_dataset_dir}")

if __name__ == "__main__":
    main()